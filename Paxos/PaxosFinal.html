<title>Paxos/Paxos.html</title>

<!------------- Start Heading -------------------------------->
<!DOCTYPE html>

<html lang="en">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet"
href="https://www.w3schools.com/w3css/4/w3.css">

<link rel="stylesheet"
href="https://fonts.googleapis.com/css?family=Roboto">

<link rel="stylesheet"
href="https://www.w3schools.com/lib/w3-theme-blue.css">

<link rel="stylesheet"
href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>
html,body,h1,h2,h3,h4,h5,h6 {font-family: "Roboto", sans-serif;}
.w3-sidebar {
  z-index: 3;
  width: 250px;
  top: 43px;
  bottom: 0;
  height: inherit;
}
/* Thick blue border */
hr.new2 {
  border: 2px solid blue;
}
/* blue border */
hr.new1 {
  border: 2px solid blue;
}
</style>


<body>


<div class="w3-top">
<div class="w3-bar w3-theme w3-top w3-left-align w3-large">
  
  <a class="w3-bar-item w3-button w3-right w3-hide-large
  w3-hover-white w3-large w3-theme-l1"
  href="javascript:void(0)"
  onclick="w3_open()">
  <i class="fa fa-bars"></i></a>

  <a href="../index.html"
  class="w3-bar-item w3-button w3-hide-small w3-hover-white">
  Distributed Algorithms
  </a>

  <a href="../table_of_contents.html"
  class="w3-bar-item w3-button w3-hide-small w3-hover-white">
  Contents
  </a>

  <a href="../cross_reference.html"
  class="w3-bar-item w3-button w3-hide-small w3-hover-white">
  Index
  </a>
    
  </div>
  </div>
  
  <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
  
<div class="w3-row w3-padding-64">
<div class="w3-twothird w3-container">


    <!--------------------------------------------------------->
  <h1 class="w3-text-teal">The Paxos Algorithm</h1>
  
  
  <!-------------------------------------------------->
  <p class="w3-text-red">

  This page describes
  <a
    href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">
  <i>Paxos</i>
  </a>,
  an algorithm by which a collection of servers execute identical
  sequences of operations proposed by clients in a faulty system.

  Agents may halt and messages may be duplicated, lost, and delivered
  out of order.
  </p>
 

  <!-------------------------------------------------->
  <h3 class="w3-text-teal">Introduction</h3>
  <!-------------------------------------------------->
    
  From
  <a href="../ConsensusImpossible/ConsensusImpossible.html">the
  FLP theorem</a>,
  
  there is no algorithm that guarantees that consensus among agents
  will be reached if messages can be lost and delayed, and if agents may
  fail and be arbitrarily slow. 

  Paxos may not terminate though it enables consensus to be reached in
  most situations.

  <p>
  The Paxos algorithm is an application of
  <a href="StableMajority.html">stable majorities in faulty
  systems</a>. 
  
  The algorithm is designed for systems in which messages may be lost; multiple
  copies of a message may be delivered; messages may not be
  delivered in the order sent; agents may be arbitrarily slow; and
  agents may stop.

  <p>
  A system consists of agents called <i>servers</i> and agents called
  <i>clients</i>.

  Clients propose operations to be executed by servers.

The sequence of operations at a server is called a <a
hef="https://en.wikipedia.org/wiki/Logging_(computing)">
<i>log</i></a> of the
  server.

  The problem is to ensure that the logs at different
  servers are replicas of each other.

  <p>
  Let \(x\) and \(y\) be sequences.
  \(x\) is a <i>prefix</i> of \(y\), written as \(x \leq y\),
  exactly when \(x\) is an initial subsequence of \(y\).

For example, the prefixes of \([a, b, c]\) are \([], [a], [a, b]\) and
\([a, b, c]\).

The algorithm is required to have the following
  invariant. For all logs \(x, y\) at servers:

<p>
\(
(x \leq y) \vee (y \leq x)
\)

<p>
For any set of logs with at least \(n\)
operations the \(n\)-th operation in all logs must be identical.

The log at one server may be behind, the same as, or ahead of the log
at another server.


<pre>
t = 0
for all servers q:
   q.v.value = None
   q.v.t = 0

while True:
   select an arbitrary client p
   t = t + positive value

   # p executes a transaction with epoch t

   # p reads variables
   select an arbitrary majority R of servers
   server_items = [q.v for q in R]
   if all server items are identical:
      propose_value = server_log_t + self.next_value(server_log_t)
   else:
      propose_value = server_log_t

   # p writes variables
   select an arbitrary subset W of servers
   for q in W:
        q.v.value = propose_value
        q.v.t = t
  </pre>

<p>Invariant</p>
For servers q, q': q.log[:-1] < q'.log[:-1]


<p>

  Associated with each client <code>p</code> is a local constant
  <code>p.VALUE</code>.

  A manager <code>q</code> has a local variable <code>q.v</code>, and
  <code>q</code> gets requests to read and write <code>q.v</code>.

  In this algorithm, <code>q.v</code> has two fields,
  <code>q.v.value</code> and <code>q.v.id</code>, which take on values
  and transaction ids of clients, respectively.

  Design an algorithm such that:
  <ol>
    <li>
    For each manager <code>q</code> of a shared variable,
  <code>q.v.value</code> is <code>None</code> or <code>p.VALUE</code>
    for some client <code>p</code>.
    </li>
    <li>

    </li>
  </ol>

  

  <p> A system has agents called <i>proposers</i> and agents called
  <i>learners</i>.

  Proposers propose values and learners come to a consensus among the
  proposed values. 

    <p>
Each learner \(L\) has a local variable \(L.v\) 
  which is initially \(None\).
  If \(L.v\) becomes any non-None value \(w\) then we
  say that \(L\) has learned \(w\).

  Associated with each proposer \(P\) is a
local constant \(P.V\) which is its proposed value. The specification
 of the algorithm is that the 
 following conditions must always hold.
<p>
<ol>
  <li>
  Learners learn only proposed values. A learner's value is either
  None or some proposer's value.
  <br>
  \(\forall \; \textrm{learners} \;  L: \quad
  (L.v = null) \; \vee \;
  (\exists \; \textrm{proposer} \: P : L.v = P.V)
  \)
  </li>
  <li>
  A learner doesn't change a value that it learns.
  If \(L.v = v*\) at some point in any computation then \(L.v = v*\) in
  succeeding states of the computation.
  <br>
  \(
  \forall \; \textrm{learners} \; L, V \neq null: \quad stable (L.v = V)
  \)
  </li>
  <li>
  All learners learn the same value.
  <br>
  \(
  \forall \; \textrm{learners} \; L, L': \quad
  (L.v = None) \vee (L'.v = None) \vee 
  ( L.v = L'.v)
  \)
  </li>
</ol>
The specification allows for the possibility that
consensus may never be reached; learners may never learn a value.

<p>
We use the algorithm for accessing shared variables to solve this
problem.
Proposers are writers of shared variables.
The algorithm ensures that if a majority of shared variables have the
same value \(v*, t*\) in an iteration then in every succeeding
iteration with epoch time \(t\) a shared variable is either not assigned a value or is assigned a
value \(v*, t\).




<!-------------------------------------------------->
  <h4 class="w3-text-teal">Algorithm Structure</h4>
<!-------------------------------------------------->

The structure of the algorithm is as follows.

<pre>
  t = 0
  for a in A: a.v, a.t = None, 0
      
  while True:
    t = t + 1
    p = any_element(P)
    read_subset = any_subset(A)

    if len(read_subset) >= len(A)/2:
        write_subset = any_subset(A)
        for a in write_subset:
           a.v, a.t = f(p, read_subset), t
</pre>

Next we describe function <code>f</code>.

If the time for all acceptors in <code>read_subset</code> is 0 then
the function returns <code>p.v</code>, the proposer's value.

Otherwise the function returns the value of the acceptor in
<code>read_subset</code> with the largest time.

<pre>
def f(p, read_subset):
   v, t = p.v, 0
   for a in read_subset:
      if a.t > t:   v, t = a.v, a.t
   return v
</pre>



<h5 class="w3-text-teal">Properties of the Sequential Program</h5>
The proofs of the following observations are straightforward.

<p>
In all states of the computation, for all acceptors \(a\),
<ol>
  <li>
  <code>a.t</code> does not decrease.
  </li>
  <li>
  <code>a.t</code> is the iteration number in which <code>a.v</code>
  was last assigned a value.
  </li>
  <li>
  \(a\)'s value is None or is a value of some proposer.
  <p>
  \(
  \forall a:  (a.v = None) \vee (\exists p: a.v = p.v)
  \)
  </li>
</ol>

  The program has the following important property which we prove
next.
<hr class="new2">
  <p style="color:blue;">
  If the write subset is a
majority in any iteration, let \(u\) be the value assigned to acceptors in
  the write subset in that iteration;
then,
<p style="color:blue;">
thereafter an acceptor's value either remains unchanged or changes to \(u\).
</p>
<hr class="new2">


<p>
The property can be written as follows. For all values \(u\),
iteration number \(n\) and majority \(M\) of acceptors:
<p>
\(
(\forall a \in M: a.v, a.t = u, n)
\)
<br>
\(
\Rightarrow
\)
<br>
\(
\forall a: (a.t < n) \vee (a.v = u)
\)
  


<!----------------------------------------------------------->
<h5 class="w3-text-teal">Proof of the property</h5>
<!----------------------------------------------------------->
Let \(M\) be a majority of acceptors, \(u\) be a value, and \(n\) be
an iteration number where all acceptors in \(M\) have value \(u\) and
time \(n\).

We will prove that in all states an acceptor's time is less than \(n\)
or its value is \(u\).

<p>
The result holds after the \(i\)-th iteration for \(i < n\) because \(a.t <
n\) in these iterations.

The result holds immediately after the \(n\)-th iteration, because
either an acceptor \(a\)'s variables are not changed in the \(n\)-th
iteration in which case \(a.t < n\) or it is modified in which case
\(a.v, a.t = u, n\).

<p>
Assume that the result holds after the first \(i \geq n\) iterations and
prove that it holds for the \(i+1\)-th.

We need only consider the case where the read subset in the iteration
is a majority, and 
in this case the read subset has an element in common
with majority \(M\).

So there is an element \(a\) in both the read subset and \(M\), and for
this element \(a.t \geq n\), and therefore \(a.v = u\).
<p>



<!----------------------------------------------------------->
<h5 class="w3-text-teal">Majorities Accept Proposed Values</h5>
<!----------------------------------------------------------->

Next, we give an example of a while loop of a
sequential program which illustrates read and write subsets.

We later show how the while loop is converted to a distributed
algorithm for a system with lossy channels.

<p>
The variables of the loop are sets <code>P</code> and <code>A</code>
called the sets of proposers and acceptors, respectively.

A proposer <code>p</code> has a field <code>p.v</code> called
<code>p</code>'s value. 

An acceptor <code>a</code> has fields <code>a.v, a.t</code> called
<code>a</code>'s value and time, respectively.

An acceptor's value is the same type as a proposer's value, and an
acceptor's time is an integer.


<p>
Initially <code>a.v, a.t = None, 0</code> for all  <code>a</code>.
The symbol <code>None</code> indicates unassigned.
The steps of the loop are as follows.
<ol>
  <li>
  Select any <code>p</code> from the set <code>P</code>
  nondeterministically.
  </li>
  <li>
  Select a subset <code>read_subset</code> of <code>A</code>
  nondeterministically.
  </li>
  <li>
  If <code>read_subset</code> is a majority of acceptors then
  select a subset, <code>write_subset</code>, of <code>A</code>
  nondeterministically, and assign values to all elements in <code>write_subset</code>.
  </li>
</ol>

<pre>
  t = 0
  for a in A: a.v, a.t = None, 0
      
  while True:
    t = t + 1
    p = any_element(P)
    read_subset = any_subset(A)

    if len(read_subset) >= len(A)/2:
        write_subset = any_subset(A)
        for a in write_subset:
           a.v, a.t = f(p, read_subset), t
</pre>

Next we describe function <code>f</code>.

If the time for all acceptors in <code>read_subset</code> is 0 then
the function returns <code>p.v</code>, the proposer's value.

Otherwise the function returns the value of the acceptor in
<code>read_subset</code> with the largest time.

<pre>
def f(p, read_subset):
   v, t = p.v, 0
   for a in read_subset:
      if a.t > t:   v, t = a.v, a.t
   return v
</pre>


<h5 class="w3-text-teal">Properties of the Sequential Program</h5>
The proofs of the following observations are straightforward.

<p>
In all states of the computation, for all acceptors \(a\),
<ol>
  <li>
  <code>a.t</code> is the iteration number in which <code>a.v</code>
  was last assigned a value.
  </li>
  <li>
  \(a\)'s value is None or is a value of some proposer.
  <p>
  \(
  \forall a:  (a.v = None) \vee (\exists p: a.v = p.v)
  \)
  </li>
</ol>

  The program has the following important property which we prove
later.
<p>
  If on any iteration a value \(u\) is  assigned to acceptors in a
write subset and the write subset is a majority,
  then thereafter any acceptor's value remains
unchanged or changes to \(u\).
  <p>


<p>
The property can be written as follows. For all values \(u\),
iteration number \(n\) and majority \(M\) of acceptors:
<p>
\(
(\forall a \in M: a.v, a.t = u, n)
\)
<br>
\(
\Rightarrow
\)
<br>
\(
\forall a: (a.t < n) \vee (a.v = u)
\)
  


<!----------------------------------------------------------->
<h5 class="w3-text-teal">Proof of the property</h5>
<!----------------------------------------------------------->
Let \(M\) be a majority of acceptors, \(u\) be a value, and \(n\) be
an iteration number where all acceptors in \(M\) have value \(u\) and
time \(n\).

We will prove that in all states an acceptor's time is less than \(n\)
or its value is \(u\).

<p>
The result holds after the first \(n-1\) iterations because \(a.t <
n\) in these iterations.

The result holds after the \(n\)-th iteration, because if an
acceptor \(a\)'s variables are modified in this iteration then
\(a.v, a.t = u, n\).

<p>
Assume that the result holds after the first \(i \geq n\) iterations and
prove that it holds for the \(i+1\)-th.

We need only consider the case where the read subset in the iteration
is a majority, and 
in this case the read subset has an element in common
with majority \(M\).

So there is an element \(a\) of read subset in which \(a.t = n\) and
\(a.v = n\).
<p>





 
<!------------------------------------->
<h4 class="w3-text-teal">Introduction of Acceptors and Timestamps</h4>

The algorithm uses agents called <i>acceptors</i> in addition to
proposers and learners. 

Each acceptor <i>A</i> has two local variables, <i>A.v</i> and
<i>A.t</i>, where <i>A.v</i> is either a special symbol null or a
proposer's value, and <i>A.t</i> is a number which is initially
\(0\).
\(A.v\) and \(A.t\) are
called \(A\)'s <i>value</i> and <i>timestamp</i>, respectively.

<p>
We will see that an agent's timestamp does not decrease, and that
timestamps specify
<a href = "../ChannelSnapshots/TimeProperties.html">
rounds in a timeline.</a>
We shall say that acceptor \(A\) has accepted \(A.v, A.t\) if \(A.v\) is not null.


<h3 class="w3-text-teal">Basic Consensus</h3>
  Before describing Paxos, we specify a subproblem, called
  <i>basic consensus</i> that is solved in Paxos. Basic consensus is
specified in terms of proposers and acceptors without learners.

<h5 class="w3-text-teal">Specification of Basic Consensus</h5>
<ol>
  <li>
Acceptors accept only proposer's values.
<p>
\(
(\exists \; \textrm{proposer} \; P: A.v = P.v) \vee (A.v = null)
\)
</li>
<li>
If there exists there exists \(vM, tM\), and a point in the
computation where all acceptors in a majority \(S\) accept \(vM, tM\)
<p>
\(
(\forall C \in S: C.v, C.t = vM, tM) \wedge (vM \neq null)
\)
<p>
then at all points in the computation, every acceptor with timestamp
\(tM\) or greater has value \(vM\)
<p>
\(
\forall \; \textrm{acceptors} \; A \; \textrm{where} \; A.t \geq tM: \;
  A.v = vM
\)

</li>
</ol>

<p>
Before giving a distributed algorithm for basic consensus we analyze a
for-loop in a sequential algorithm. The sequential algorithm gives
insight into the distributed version.

<!------------------------------------->
<h4 class="w3-text-teal">A Sequential Simulation of the Basic
Consensus Algorithm</h4>

The sequential program consists of a for-loop with loop index \(k\).
Initially, \(A.v, A.t = null, 0\) for all acceptors \(A\).

<p>
The loop body selects an arbitrary proposer <code>P</code>, and
arbitrary subsets <code>R, W</code> of acceptors; these selections simulate
nondeterminism and faults in the distributed system.

In the loop, agents are treated as shared variables.
For example, the loop reads and writes <code>A.v, A.t</code> for an
acceptor <code>A</code>.


<h5 class="w3-text-teal">Loop Body of the Basic Consensus
Algorithm</h5>

The loop body for the \(k\)-th iteration is as follows.

<ol>
  <li>
  Select an arbitrary proposer <code>P</code>, and arbitrary subsets
  <code>R,W</code> of acceptors.
  </li>
  <li>
  If <code>R</code> is not a majority then take no action.
  </li>
  <li>
  If <code>R</code> is a majority then for all acceptors \(A\) in
  <code>W:  A.v, A.t = f(R), k</code>
  </li>
</ol>

<h5 class="w3-text-teal">Function f</h5>
<ol>
  <li>
If <code>A.v = null</code> for all <code>A</code> in <code>R</code>
  then <code>f(R) = P.v</code>;
  </li>
  <li>
  else <code>f(R) = C.v</code>
  <br>
  where <code>C</code> is a member of
  <code>R</code> with the largest  timestamp, i.e.,
  <br>
  <code>
  C.t = max({r.t | r in R})
  </code>
  </li>
</ol>


<!-------------------------------------------->
<h3 style="color:red;">Example of Sequential Simulation of Basic Consensus</h3>
This example has acceptors A0, A1, A2 and two proposers with values X
and Y. Each row of the table shows the values of variables on the
\(k\)-th iteration where \(k = 0\) represents initial conditions. The
symbol <i>-</i> is used instead of \(null\).

<pre>
k   P.v   R      W      A0         A1         A2
0   -     -      -      (-, 0)     (-, 0)     (-, 0)
1   X     0      0,1    (-, 0)     (-, 0)     (-, 0)
2   X     0,1    1      (-, 0)     (X, 2)     (-, 0)
3   Y     0,2    0      (Y, 3)     (X, 2)     (-, 0)
4   Y     1,2    2      (Y, 3)     (X, 2)     (X, 4)
5   X     0,1    0,2    (Y, 5)     (X, 2)     (Y, 5)
6   X     1,2    1      (Y, 5)     (Y, 6)     (Y, 5)
7   X     0,1    2      (Y, 5)     (Y, 6)     (Y, 7)
8   X     1      1,2    (Y, 5)     (Y, 8)     (Y, 8)
</pre>

The columns with headings \(P.v, R, W\) specify the proposer, and
subsets of acceptors that are read, \(R\), and written, \(W\). For
example, on iteration 1, \(P.v = X\), \(R = \{0\}\), \(W = \{0,1\}\). The
values of the acceptors at the end of iteration \(i\) are shown in the row
where \(k = i\).

<h5 style="color:red;">Points to Note in the Example</h5>
The majority of \(A.v\) for acceptors \(A\) after iteration 4 is \(X\)
because \(A1, A2 = (X, 2), (X, 4)\). The final majority, however, is
\(Y\); see row 6.

<p>
In row 4, \(A1.t \neq A2.t\), and that's why the majority value does
not remain \(X\).
In row 5, the majority of acceptors have identical \(A.v\)
<i>and</i> \(A.t\), and so, from row 5 onwards, the acceptor with the
largest timestamp has value \(Y\).


<!-------------------------------->
<h3 class="w3-text-teal">Sequential
Simulator satisfies Specification of Basic Consensus</h3>

The proof of the invariance of part 1 -- acceptors accept only
proposer's values -- is
straightforward. Next, we prove part 2.

<p>
Consider the case where at the end of the \(k\)-th iteration
there exists \(vM \neq null\) such
that for all \(C\) in a majority \(S\) of acceptors:
<p>
\(
C.v, C.t = vM, k
\)
<p>
Then at the end of the \(k\)-th iteration conditions 1 and 2 hold:
<p><i>Condition 1</i>:
Acceptors have value
\(vM\) or earlier timestamps:
<p>
for all acceptors \(A\):
\(
  ( A.v = vM) \vee (A.t < k)
\)
<p><i>Condition 2</i>:
Values of all acceptors in \(S\) are \(vM\), and their timestamps are
at least \(k\).
<p>
for all acceptors \(C\) in \(S\):
\(
  (C.v = vM) \wedge (C.t \geq k)
\).

<p>
We will show that if conditions 1 and 2 hold at the beginning of the \(i\)-th
iteration, for \(i > k\), then they hold at the end of that iteration,
and therefore continue to hold thereafter. We will prove that if both
conditions hold and \(R\)
is a majority then \(f(R) = vM\) and so both conditions continue to hold.

<p>
Two majorities \(R\) and \(S\) have an element in common. Let
\(X\) be common to \(R\) and \(S\).

From condition 2,  \(X.t \geq k\).

Therefore, the element in \(R\) with the largest timestamp
  has a timestamp of \(k\) or greater.

<p>
From condition 1, if \(A.t \geq k\) then \(A.v = vM\).

So, the element in \(R\) with the largest timestamp has value
\(vM\), and therefore \(f(R) = vM\).
  




<!------------------------------------->

<h3 class="w3-text-teal">
A Round of a Distributed Basic Consensus Algorithm
</h3>

Next, we describe a computation of a Paxos algorithm where the computation
is a sequence of rounds, and each round is a distributed version of
the sequential simulation given earlier. The computation has
timelines for each of the proposers and acceptors. The reason for the
names of messages -- <code>prepare</code>, <code>promise</code>,
<code>accept</code> -- will become clear later.

<h4 class="w3-text-teal">Round \(k\) of the Computation</h4>

<p>
A round starts with the selection of a single proposer \(P\).
Later, we describe how the proposer is selected.
Round \(k\) has four steps which are as follows.

<ol>
  <li>
  \(P\) sends a message <code>prepare(k)</code> to all acceptors.
  <p>
  </li>
  <li>
  Each acceptor \(A\) that receives a <code>prepare(k)</code>
  message from \(P\) sends a reply <code>promise(k, A.v, A.t)</code>
  to \(P\).
  <p>
  </li>
  <li>
  If \(P\) receives <code>promise(k, _, _)</code> messages from a
  majority \(R\) of acceptors then \(P\) sends <code>accept(k,
  f(R))</code> messages to all acceptors. The symbol <code>-</code>
  represents an arbitrary value.
  <p>
  </li>
  <li>
  If an acceptor \(A\) receives an <code>accept(k, v)</code>
  message  then <code>A.v, A.t = v, k</code>.
  <p> 
  </li>
</ol>

<h5 class="w3-text-teal">Four Steps of Round \(k\) in Pseudo Code</h5>

<pre>
// Proposer P. Step P.1.
send prepare(k) to all acceptors

// Acceptor A. Step A.1: 
upon arrival of prepare() from a proposer P:
   send promise(k, A.v, A.t) to P

// Proposer P. Step P.2.
wait until
   end of round or
   arrival of promise(k, -, -) messages from any majority
   R of acceptors;

if promise messages are received from a majority R:
   send accept(k, f(R)) to all acceptors

// Acceptor A. Step A.2. 
upon arrival of accept(k, v): A.v, A.t = v, k
</pre>


<!------------------------------------->
<h4 class="w3-text-teal">Rounds in the Sequential Simulation and
Distributed Computation are Equivalent</h4>

Next we show that each round of the simulation is equivalent to each
round in the timeline.

Therefore a property that holds after a
round in the simulation also holds in the corresponding round of the
timeline.

So, the round-based timeline satisfies the specification of basic consensus.

<p>
In the distributed algorithm messages may get lost.
So proposer \(P\) receives
<code>promise</code> from an arbitrary subset \(R\) of acceptors.
Likewise, an arbitrary
subset \(W\) of acceptors receive <code>accept</code> messages.

<p>
  eading <code>A.v, A.t</code> from acceptors \(A\) in \(R\)
  in the sequential simulation is equivalent to receiving 
<code>promise(k, A.v, A.t)</code> from acceptors \(A\) in \(R\) in the
  distributed algorithm.


  Similarly, assigning <code>v, k</code> to each acceptor \(A\) in
  \(W\) in the sequential simulation
  is equivalent to each acceptor \(A\) in \(W\) receiving <code>accept(k,
v)</code> in the distributed algorithm.




<h3 class="w3-text-teal">The Distributed Basic Consensus Algorithm</h3>
Next we described a distributed basic consensus algorithm.
The algorithm obeys the rules for assigning
<a href="../ChannelSnapshots/TimeProperties.html"></a>
rounds to events.
Therefore, the causality graph of a computation of the distributed
algorithm is the same
as the causality graph of a computation that occurs as a sequence of
rounds.

A proof about states of agents in any computation
is also a proof for all computations with the same causality graph.
See the
<a href="../ChannelSnapshots/TimeProperties.html">section:
<i>A Technique for Analyzing Algorithms in Systems
with Message Loss</i>.
</a>


<!---------------------------------------->
<p>
A proposer \(P\) has a unique id \(P.id\) and a local integer variable
\(n\). The pair \([P.n, P.id]\) is a proxy for the iteration index
\(k\) of the sequential simulator, and the round id of the distributed
algorithm executing in rounds.

<p>
The timestamp for a message is its first field.

In the algorithm given below this field has value \(k\).

The time of an acceptor \(A\) is \(A.t\), and the time of proposer
\(P\) is \(P.t\).


<h5 class="w3-text-teal">The Basic Consensus Algorithm</h5>
<pre>
// Proposer P. Step P.1. 
P.n = P.n + any positive value
P.t = [P.n, P.id]
// P.t represents k in rounds of the timeline
send prepare(P.t) to all acceptors

// Acceptor A. Step A.1: 
upon arrival of prepare(k) at A from a proposer P:
   if k > A.t:
      A.t = k
      send promise(k, A.v, A.t) to P

// Proposer P. Step P.2.
wait until timeout or:
     arrival of promise(k, -, -) from any majority
     R of acceptors where k = P.t

If timeout: return to step P.1
else: send accept(k, f(R)) to all acceptors
      
// Acceptor A. Step A.2. 
upon arrival of accept(k, v):
    if k > A.t: A.t, A.v = k, v
</pre>


<h3 class="w3-text-teal">The Existence of a Computation Executing in Rounds</h3>
We will prove that given any computation \(C\) of the Paxos algorithm
there exists a timeline, with the causality
graph of timelines the Paxos algorithm, in which events occur in rounds:
All events at agents when the agents' time equal \(k\) occur before
all events at agents when the agents' time exceeds \(k\).


<h3 class="w3-text-teal">Theorem</h3>
Given any computation \(C\) of an algorithm in which messages may be lost,
where the algorithm follows the rules for assigning rounds, there
exists a computation \(C'\):
<ol>
  <li>
  with the same causality graph as \(C\) and
  </li>
  <li>
  in which, for all \(k\), events with round id \(k\) occur before
  events with round 
  ids that exceed \(k\), and 
  </li>
  <li>
  messages sent in a round are received in the same round.
  </li>
</ol>

The proof follows from the rules for assigning round ids to events and
messages.

<ol>
  <li>
  A message sent in an event with round id \(k\) has
  timestamp \(k\).
  </li>
  <li>
  A message with round id \(k\) is discarded if the message arrives
  at the receiver when the receiver's round id exceeds \(k\).
  </li>
  <li>
  When a message with timestamp \(k\) arrives at an agent with
  timestamp \(k\) or greater the message is either discarded or the
  receiver's timestamp is set to \(k\).
  </li>
</ol>
  


<figure>
    <img src="PaxosFigures/PaxosFigures.012.jpeg" alt="Fig2" style="width:100%">
    <figcaption>Fig.1: Timelines</figcaption>
</figure>

<figure>
    <img src="./PaxosProof/PaxosProof.008.jpeg" alt="Fig4" style="width:80%">
    <figcaption>Fig.2: Example of computation in rounds</figcaption>
</figure>



<h4 class="w3-text-teal">Learners</h4>
The algorithm for learners is simple:
A learner determines that the consensus value is <code>V</code> if the
learner receives <code>accept(T, V)</code> messages from a majority of
acceptors with identical values of <code>(T, V)</code>.
We ignore learners in the algorithm description, and restrict
attention to whether and when a majority of
acceptors sends <code>accept(T, V)</code> messages with identical
values of <code>(T, V)</code>.





    <!--------------------------------------------------------->
  <h2 class="w3-text-teal">OLD OLD OLD</h2>
<!--------------------------------------------------------->




    <!--------------------------------------------------------->
  <h2 class="w3-text-teal">The Problem</h2>
  <!--------------------------------------------------------->
In each iteration of the sequential algorithm, given above, exactly
  one writer reads and then writes all shared variables.

We will implement the following variant of the sequential algorithm
on a faulty distributed system.

We call the variant <i>Seq</i>.

<p>
<i>Seq</i> initializes variables and then executes a loop.

A single writer <code>p</code> is selected nondeterministically in each
iteration.

The selected writer <code>p</code>
reads and then writes subsets of shared variables.

No writer, other than <code>p</code>,
accesses shared variables in this iteration.

<p>
The writer <code>p</code>, selected on an iteration, determines a
value <code>p.t</code>, called the <i>iteration id.</i>

Later we give rules that ensure that the id of each iteration is unique.


<p>
The manager <code>q</code> of a shared variable has a field
<code>q.var</code> where <code>q.var</code> is a tuple <code>(v,
t)</code> where
<code>v</code> is the value of the shared variable and 
<code>t</code> is the id of the most recent iteration in which
<code>v</code> was assigned a value.

<p>
Each writer <code>p</code> has a local variable <code>p.copy</code>, a
dictionary, where <code>p.copy[q]</code> contains a (possibly stale)
copy of <code>q.var</code>.

Each writer <code>p</code> also has a local variable <code>p.t</code>
which is the id of the most recent iteration in which <code>p</code>
was selected as the writer that accesses shared variables.

<p>
We describe the steps of the sequential program later.



<!---------------------------------------------------->
<h4 class="w3-text-teal">Sequential Program Seq</h4>
<!---------------------------------------------------->
    <pre>
for p in P: p.t = 0
for q in Q: q.var = (None, 0)

while True:
   select a writer p
   p.t = p.t + pos()

   p.copy = {}
   for q in read_set: p.copy[q] = q.var

   if g(p):
      for q in write_set: q.var = (f(p, q), p.t)
</pre>

<code>None</code> is a value that indicates unassigned.

<code>pos</code> is a function that returns a positive value.

The value returned by <code>pos</code> impacts the performance, but not
the correctness of the algorithm.


<p>
<code>read_set</code> and <code>write_set</code> are subsets of
managers of shared variables.

These subsets are selected nondeterministically.


<p>
The iteration id determined by a selected writer <code>p</code> is a
pair <code>(n,p.id)</code> where 
<code>n</code> is a number, and 
<code>p.id</code> is the id of <code>p</code>.

So, different writers cannot determine the same iteration id.

When <code>p</code> starts an iteration it increases <code>p.t</code>.

So, different iterations in which <code>p</code> is the selected
writer have different ids.

<p>
(We don't include <code>p.id</code> explicitly in the algorithm
description so that we can focus on more important aspects.)


<p>
<code>p</code> reads shared variables in <code>read_set</code>, and
<code>p</code> writes shared variables in <code>write_set</code> only 
if condition <code>g(p)</code> holds.

For example, an algorithm may write shared variables only if 
<code>read_set</code> is not empty.

<p>
In the statement that writes shared variables, <code>f(p,q)</code> is
assigned to the value field, 
and <code>p.t</code> is assigned to the iteration id field of
<code>q.var</code>.

<p>
The nondeterministic statements of Seq 
are the statements that select <code>p</code>,
<code>read_set</code> and <code>write_set</code> in each iteration;
different selections of these variables result in different executions
of Seq.
An instance of Seq is a deterministic algorithm with a given selection of these
variables.


<!-------------------------------------------------->
<h3 class="w3-text-teal">Problem Specification</h3>
<!-------------------------------------------------->

Design a distributed algorithm in which agents are writers, managers of shared
variables and observers, and in which an observer receives
a message <code>m</code> 
from a manager <code>q</code> of a shared variable only if
<code>q.var = m</code> at some point in the execution of an instance
of Seq.


<p>
Observers cannot distinguish between messages that they receive in the
distributed algorithm and values of shared variables in Seq.

<p>
In the observer pattern, agents sends descriptions of events to
observers which construct the dataflow of a computation.

In our algorithm, copies of shared variables are sent to observers.
x
Messages may, however, be lost, duplicated and delivered out of order.

What can we deduce from the messages that observers receive?

<p>
If an observer receives <code>(v, t)</code> and <code>(v', t)</code>
from <code>q</code> and <code>q'</code>, respectively, 
then we know that <code>q.var</code> and <code>q'.var</code> were
assigned values <code>(v, t)</code> and <code>(v', t)</code>,
respectively, in the same iteration -- the iteration with unique id
<code>t</code>. 


<!-------------------------------------------------->
<h3 class="w3-text-teal">Performance Issues</h3>
<!-------------------------------------------------->





<p>
The events in the dataflow of an iteration are as follows.

<ol>
  <li>
  A single writer <code>p</code> is selected.
  </li>
  <li>
  The epoch time <code>p.t</code> is increased and the resulting epoch
  time uniquely identifies the iteration -- no other iteration has the
  same epoch time.
  </li>
  <li>
  <code>p</code> sends read requests to shared variables.
  </li>
  <li>
  A read request may get lost, or the manager may ignore the request,
  or the manager may send a reply.
  </li>
  <li>
  A reply may get lost, or <code>p</code> may ignore the reply, or
  <code>p</code> may copy the reply into a local variable and send
  write requests if <code>g(p)</code> holds.
  </li>
  <li>
  A write request may get lost, or a manger may ignore the request, or
  the manager may assign the value specified in the request to the
  shared variable.
  </li>
</ol>


<p>
Ignoring a message has the same effect as losing the message -- the
message is removed from the channel without changing agent states.

<p>
<code>read_set</code> is the set of managers that send replies that
<code>p</code> copies in step 5.
<code>write_set</code> is the set of managers that assign
values to shared variables specified in write requests in step 6.



<p>
<i>The Problem</i>:

Determine how to select a single writer <code>p</code> for each
iteration and which requests and replies to ignore and which to process so that:
<hr class="new2">
  <p style="color:blue;">
Values assigned to shared variables in the distributed
and sequential algorithms are identical.
</p>
<hr class="new2">




<h5 class="w3-text-teal">Message Types</h5>
<ol>
  <li>
  A read request is an instance of a class <code>ReadRequest</code>
  and has a single field <code>t</code> which is the epoch  time of
  the request.
  </li>
  <li>
  A reply is an instance of a class <code>Reply</code> and has fields
  <code>var</code> and <code>t</code> which are a copy of the shared
  variable and the epoch time of the reply, respectively.
  </li>
  <li>
  A write request is an instance of a class <code>WriteRequest</code>
  and has fields <code>var</code> and <code>t</code> which
  are the value to be written into the shared variable and the epoch
  time of the request, respectively. 
  </li>
</ol>


<!--Start Footer--------------------------------------->

    <hr class="new1">
      <p>K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology</p>
    
</footer>
    


<!-- END MAIN -->
</div>


</body>
</html>
